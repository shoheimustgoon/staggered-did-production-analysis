### 1\. `main_hybrid_utilization.py` ã®æœ€çµ‚ã‚³ãƒ¼ãƒ‰

ã¾ãšã€ãƒ‘ãƒ³å·¥å ´ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã«å®Œå…¨ã«æ²¿ã£ãŸã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

```python
# -*- coding: utf-8 -*-
"""
Hybrid Utilization Estimation (Proxy-based Imputation)
Logic: Learns relationship between Oven Usage/Cleaning Time and Loaves Baked (Production)
to estimate utilization during data-scarce periods.

Author: Go Sato
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os

# ==========================================
# 1. Dummy Data Generator (Simulates Missing Data)
# ==========================================
def generate_hybrid_dummy_data(n_tools=3, start_date='2024-01-01', months=6):
    """Generates Loaves Baked Data with Gaps and Oven Usage Log (Proxy Data)."""
    tools = [f'OVEN_{i:02d}' for i in range(1, n_tools + 1)]  # ãƒ„ãƒ¼ãƒ«ã‚’ã‚ªãƒ¼ãƒ–ãƒ³ã«å¤‰æ›´
    end_date = pd.to_datetime(start_date) + pd.DateOffset(months=months)
    date_range = pd.date_range(start_date, end_date, freq='D')
    
    prod_rows = []
    clean_rows = [] # Proxy/Consumption Logç”¨

    for t in tools:
        util_factor = np.random.uniform(0.5, 1.2)
        base_production = 500 * util_factor
        
        # --- 1. Output Data (Loaves Baked: ãƒ‘ãƒ³ã®è£½é€ æ•°) ---
        for d in date_range:
            daily_loaves = int(np.random.normal(base_production, 20))
            if np.random.rand() < 0.3 and d < pd.to_datetime('2024-03-01'):
                # æ—©æœŸã®æœŸé–“ã§ãƒ‡ãƒ¼ã‚¿æ¬ æã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                prod_rows.append({'Date': d, 'Tool': t, 'Loaves_Baked': np.nan})
            else:
                prod_rows.append({'Date': d, 'Tool': t, 'Loaves_Baked': max(0, daily_loaves)})

        # --- 2. Proxy Data (Material Consumption Log: ææ–™æ¶ˆè²»é‡) ---
        # ææ–™æ¶ˆè²»é‡ãŒæœˆæ¬¡ã§è¨˜éŒ²ã•ã‚Œã‚‹ã¨ä»®å®š
        monthly_dates = pd.date_range(start_date, end_date, freq='MS')
        for md in monthly_dates:
            # Proxyã¨ãªã‚‹ææ–™æ¶ˆè²»é‡ï¼ˆä½¿ç”¨æ™‚é–“ã§ã¯ãªã„ï¼‰
            material_consumed = 500 * util_factor * np.random.uniform(0.9, 1.1)
            clean_rows.append({'Date': md + timedelta(hours=1), 'Tool': t, 'Material_Consumed_Kg': material_consumed}) # RF_Time_Hrs -> Material_Consumed_Kg

    df_prod = pd.DataFrame(prod_rows)
    df_clean = pd.DataFrame(clean_rows)
    return df_prod, df_clean

# ==========================================
# 2. Hybrid Estimation Logic
# ==========================================
def estimate_missing_utilization(df_prod, df_clean):
    print("\n[Step 1] Learning Production Coefficient from Proxy Data...")
    
    # 1. æœˆæ¬¡é›†è¨ˆ (Material Consumption & Loaves)
    df_prod['Month'] = df_prod['Date'].dt.to_period('M').dt.to_timestamp()
    df_clean['Month'] = df_clean['Date'].dt.to_period('M').dt.to_timestamp()
    
    df_monthly_prod = df_prod.groupby(['Tool', 'Month'])['Loaves_Baked'].sum().reset_index()
    df_monthly_clean = df_clean.groupby(['Tool', 'Month'])['Material_Consumed_Kg'].sum().reset_index()
    
    df_merged = pd.merge(df_monthly_prod, df_monthly_clean, on=['Tool', 'Month'], how='outer')
    
    # 2. ä¿‚æ•°å­¦ç¿’ (Coefficient = Material / Loaves) - ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã‚‹æœŸé–“ã§å­¦ç¿’
    # Coeff: 1 Loaf (Unit) ç”Ÿç”£ã™ã‚‹ã®ã«å¿…è¦ãªææ–™æ¶ˆè²»é‡ (Proxy)
    df_merged['Coefficient'] = df_merged['Material_Consumed_Kg'] / df_merged['Loaves_Baked'].replace(0, np.nan)
    
    # Toolã”ã¨ã®å¹³å‡ä¿‚æ•° (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿)
    tool_coeffs = df_merged.dropna(subset=['Coefficient']).groupby('Tool')['Coefficient'].mean().to_dict()
    global_coeff = pd.Series(tool_coeffs).median()
    
    print(f"  Learned Global Coefficient (Median Kg/Loaf): {global_coeff:.4f}") 

    # 3. æ¬ ææœŸé–“ã®æ¨å®š (Imputation)
    df_imputed = df_merged.copy()
    
    for index, row in df_imputed.iterrows():
        # æ¨å®šæ¡ä»¶: Loaves_BakedãŒæ¬ æ (NaN) ã‹ã¤ Material_Consumed_KgãŒã‚ã‚‹å ´åˆ
        if pd.isna(row['Loaves_Baked']) and pd.notna(row['Material_Consumed_Kg']):
            tool = row['Tool']
            material_consumed = row['Material_Consumed_Kg']
            coeff = tool_coeffs.get(tool, global_coeff)
            
            # Estimated Production = Material Consumed / Coefficient
            if coeff > 0:
                estimated_loaves = material_consumed / coeff
                df_imputed.loc[index, 'Loaves_Baked_Imputed'] = estimated_loaves
            else:
                df_imputed.loc[index, 'Loaves_Baked_Imputed'] = 0
        else:
            # æ¬ æã—ã¦ã„ãªã„æœŸé–“ã¯å®Ÿæ¸¬å€¤ã‚’ä½¿ç”¨
            df_imputed.loc[index, 'Loaves_Baked_Imputed'] = row['Loaves_Baked']
            
    print(f"  Total Imputed Values: {df_imputed['Loaves_Baked_Imputed'].isna().sum()} (Should be 0 if clean)")
    return df_imputed[['Tool', 'Month', 'Loaves_Baked', 'Loaves_Baked_Imputed', 'Material_Consumed_Kg']]

# ==========================================
# 3. Main Execution
# ==========================================
if __name__ == "__main__":
    df_prod_raw, df_clean_log = generate_hybrid_dummy_data()
    
    print("--- 1. Raw Data Status (Loaves Baked) ---")
    print(df_prod_raw[df_prod_raw['Loaves_Baked'].isna()].head())
    print(f"\nTotal Missing Loaves Baked Counts: {df_prod_raw['Loaves_Baked'].isna().sum()}")
    
    df_results = estimate_missing_utilization(df_prod_raw, df_clean_log)
    
    print("\n--- 2. Imputation Results (Monthly) ---")
    print(df_results.head(10))
    
    print("\n[Conclusion]")
    print(f"The 'Loaves_Baked_Imputed' column now contains estimated values where 'Loaves_Baked' was missing, based on the Material_Consumed_Kg proxy.")
```

-----

### 2\. `README.md` æœ€çµ‚çµ±åˆç‰ˆ

ã“ã®ã‚³ãƒ¼ãƒ‰ (`main_hybrid_utilization.py`) ã¨ã€ã“ã‚Œã¾ã§ã®å…¨ã¦ã®åˆ†æã‚’ç¶²ç¾…ã—ãŸæœ€çµ‚ç‰ˆ `README.md` ã§ã™ã€‚

````markdown
# Impact Analysis of Manufacturing Equipment Upgrade (Survival, Staggered DiD & Event Study)

> **[ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã®èª¬æ˜ã¯ã“ã¡ã‚‰ (Click here for Japanese Description)](#japanese-description)**

## ğŸ“– Overview
This project is a Python-based analytical framework designed to verify the effectiveness of new equipment components in a manufacturing environment. It addresses complex real-world conditions such as **staggered installation dates** and **varying equipment utilization rates**.

The core survival technique (Cox PH/AFT) is directly applicable to medical/pharmaceutical analysis (Time-to-Event), such as assessing drug efficacy and patient survival.

It includes three complementary analytical approaches:
1.  **Survival Analysis:** KMF, Cox PH, and AFT models using utilization-corrected metrics.
2.  **Static DiD Analysis:** Standard DiD (OLS/GLM) for overall effect quantification.
3.  **Dynamic DiD Analysis:** Event Study for trend visualization.

To ensure data confidentiality, this project uses a **"Bread Factory" analogy** to demonstrate the analytical logic without exposing sensitive production data.

---

## ğŸ¥ The Analogy: The Bread Factory

### The Context
- **The Factory:** A large factory producing bread with 20-40 industrial ovens (Tools).
- **The Upgrade:** A new "AI Temperature Controller" was installed to prevent bread from burning (Failures).
- **The Goal:** To statistically prove that the new controller reduces the failure rate.

### The Challenges
1.  **Varying Utilization:** Oven A runs 24/7, while Oven B runs only 2 hours. Simple **MTBF** (Mean Time Between Failures) is unfair.
    - *Solution:* We derive **Normalized MTBF** using **Production Count (Loaves Baked)**. We normalize metrics using **"Effective Denominator"** (Material Consumption / Loaves Baked).
2.  **Staggered Installation:** Controllers were installed at different times (Jan, Mar, Jun...).
    - *Solution:* We align data using **Relative Time ($K$)** and use Staggered DiD / Event Study models.

#### The Data Scarcity Problem (Hybrid Estimation)
We initially ran the DiD analysis assuming all ovens were operating consistently. **However, the initial results were unstable and unreliable**, proving our fundamental premise was flawed due to varying utilization.

To solve the data scarcity that arose when measuring "Loaves Baked" directly, we developed a proxy estimation system: We used secondary metrics, such as **"Oven Cleaning Frequency"** or **"Energy Consumption" (RF time equivalent)**, to **estimate** the missing production counts. This process, handled by the utility script (`main_hybrid_utilization.py`), ensures utilization correction is robustly applied across the entire historical dataset.

---

## ğŸ›  Included Scripts & Methodology

### 1. `main_hybrid_utilization.py` (Data Utility)
**Purpose:** Solves the data scarcity challenge by imputing missing output volumes.
- **Logic:** Learns the relationship between **Material Consumption (Proxy Input)** and **Loaves Baked (Output)** during data-rich periods. This ratio is then used to estimate the missing 'Loaves Baked' when only the proxy input is available.
- **Output:** Provides the necessary **Utilization-Corrected Data** for the downstream analysis scripts (2, 3, & 4).

### 2. `main_survival_analysis.py` (Survival Analysis)
Focuses on the duration metric and risk modeling. This script uses **MTBF (Mean Time Between Failures)** corrected for utilization as the primary duration metric.

#### Right-Censoring Logic
The analysis incorporates **Right-Censoring**, which is vital for unbiased results. Censoring occurs when a unit (oven) is still functional when the observation period ends. In the analysis, these data points are included with an `Event = 0` flag to correctly estimate the population risk and survival curve.

| Model | Goal (English) | Role in this Project (Japanese) |
| :--- | :--- | :--- |
| **KMF** (Kaplan-Meier) | Estimates the **Survival Function** (non-parametric). | **è¦–è¦šçš„ãªæ¯”è¼ƒ:** æ–°æ—§ãƒ‡ãƒã‚¤ã‚¹é–“ã®MTBFï¼ˆæ•…éšœé–“éš”ï¼‰ã®åˆ†å¸ƒã‚’ã‚°ãƒ©ãƒ•ã§ç¤ºã—ã¾ã™ã€‚ |
| **Cox PH** (Proportional Hazards) | Estimates the **Hazard Ratio** (risk ratio, semi-parametric). | **ãƒªã‚¹ã‚¯ä½æ¸›ã®å®šé‡åŒ–:** æ–°ãƒ‡ãƒã‚¤ã‚¹å°å…¥ã«ã‚ˆã‚‹**æ•…éšœãƒªã‚¹ã‚¯ã®æ¸›å°‘ç‡**ã‚’æ¨å®šã—ã¾ã™ã€‚ |
| **Weibull AFT** (Accelerated Failure Time) | Estimates the **Acceleration Factor** (lifespan ratio, parametric). | **å¯¿å‘½ã®å®šé‡åŒ–:** æ–°ãƒ‡ãƒã‚¤ã‚¹å°å…¥ã«ã‚ˆã‚‹**MTBF**ã®å»¶é•·ç‡ã‚’æ¨å®šã—ã¾ã™ã€‚ |

### 3. `main_analysis.py` (Static DiD Analysis)
Focuses on quantifying the overall Average Treatment Effect (ATT) using the standard **Two-Way Fixed Effects (TWFE)** structure.

#### Two-Way Fixed Effects (TWFE) Details
TWFE models are used for panel data to control for unobserved confounding variables.

| å›ºå®šåŠ¹æœã®ç¨®é¡ | åˆ¶å¾¡ã™ã‚‹æœªè¦³æ¸¬ã®è¦å›  | ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®å¯¾å¿œ |
| :--- | :--- | :--- |
| **Entity Fixed Effects** | Unit-specific factors (e.g., environment, initial performance) | **è£…ç½®**ã”ã¨ã®å›ºå®šåŠ¹æœï¼ˆ`C(Tool)` ã¾ãŸã¯ `C(group)`ï¼‰ã«ã‚ˆã‚Šåˆ¶å¾¡ |
| **Time Fixed Effects** | Factors common across all units at a given time (e.g., site-wide power failure, component delays) | **æ™‚é–“**ã”ã¨ã®å›ºå®šåŠ¹æœï¼ˆ`C(Month)` ã¾ãŸã¯ `C(Date)`ï¼‰ã«ã‚ˆã‚Šåˆ¶å¾¡ |

- **Staggered DiD Implementation:** The core staggered logic (unit-specific `Post` variable timing) is applied to both OLS and GLM.
- **Continuous Outcomes (MTBF):** Uses **OLS** regression.
- **Count Outcomes (Rate):** Uses **Negative Binomial GLM** with the `log(Production)` offset for utilization normalization.

### 4. `main_event_study.py` (Dynamic Analysis)
Focuses on visualizing the timing of the effect and checking the Parallel Trend assumption.
- **Event Study (PanelOLS):** Visualizes the causal impact trend before and after installation.
- **Parallel Trend Check:** Verifies if the pre-installation trend ($K < 0$) is flat (validating the causal assumption).

---

## ğŸ’» Usage

### Running the Analysis
The project uses a three-step workflow, starting with data imputation.

#### Step 1: Run Hybrid Imputation (Utility)
```bash
python main_hybrid_utilization.py
````

#### Step 2: Run Survival Analysis (Risk Modeling)

```bash
python main_survival_analysis.py
```

#### Step 3: Run DiD Analysis (Causal Effect)

```bash
python main_analysis.py
```

#### Step 4: Run Dynamic Analysis (Event Study)

```bash
python main_event_study.py
```

-----

## ğŸ‘¨â€ğŸ’» Author

**Go Sato**
Data Scientist | AI Department, Semiconductor Equipment Manufacturer
Specializing in Causal Inference, Survival Analysis, and Reliability Engineering.

<br>
<br>
<br>

-----

## Japanese Description

-----

# è£½é€ è£…ç½®ã®å°å…¥åŠ¹æœåˆ†æï¼ˆç”Ÿå­˜åˆ†æã€Staggered DiD & ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒ‡ã‚£ï¼‰

## ğŸ“– æ¦‚è¦

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€è£½é€ ç¾å ´ã«ãŠã‘ã‚‹æ–°è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å°å…¥åŠ¹æœã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®Pythonåˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚**å°å…¥æ™‚æœŸãŒè£…ç½®ã”ã¨ã«ç•°ãªã‚‹ç‚¹**ã‚„ã€**è£…ç½®ã”ã¨ã®ç¨¼åƒç‡ã®ã°ã‚‰ã¤ã**ã¨ã„ã£ãŸã€å®Ÿä¸–ç•Œã®è¤‡é›‘ãªæ¡ä»¶ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

**ã“ã®ç”Ÿå­˜åˆ†æã®æ ¸ã¨ãªã‚‹æ‰‹æ³•ï¼ˆCox PH/AFTï¼‰ã¯ã€è–¬åŠ¹ã®åˆ†æã‚„æ‚£è€…ã®ç”Ÿå­˜æœŸé–“æ¨å®šãªã©ã€åŒ»ç™‚ãƒ»è£½è–¬åˆ†é‡ï¼ˆTime-to-Eventï¼‰ã«ã‚‚ç›´æ¥å¿œç”¨å¯èƒ½ãªã‚¹ã‚­ãƒ«ã§ã™ã€‚**

ä»¥ä¸‹ã®4ã¤ã®è£œå®Œçš„ãªåˆ†æã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å«ã¿ã¾ã™ï¼š

1.  **ãƒ‡ãƒ¼ã‚¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£:** æ¬ æã—ãŸç¨¼åƒç‡ãƒ‡ãƒ¼ã‚¿ã‚’æ¨å®šã—ã€è£œå®Œã™ã‚‹ã€‚
2.  **ç”Ÿå­˜æ™‚é–“åˆ†æ:** KMFã€Cox PHã€AFTãƒ¢ãƒ‡ãƒ«ã‚’ç¨¼åƒç‡è£œæ­£ã•ã‚ŒãŸæŒ‡æ¨™ã§å®Ÿè¡Œã€‚
3.  **é™çš„DiDåˆ†æ:** æ¨™æº–çš„ãªDiDï¼ˆOLS/GLMï¼‰ã«ã‚ˆã‚‹å…¨ä½“åŠ¹æœã®å®šé‡åŒ–ã€‚
4.  **å‹•çš„DiDåˆ†æ:** ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒ‡ã‚£ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¦–è¦šåŒ–ã€‚

æ©Ÿå¯†ä¿æŒã®ãŸã‚ã€æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯\*\*ã€Œãƒ‘ãƒ³å·¥å ´ã€ã®ãŸã¨ãˆè©±\*\*ã‚’ç”¨ã„ã¦ã€å®Ÿéš›ã®è£½é€ ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã‚ãšã«åˆ†æãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¨¼ã—ã¦ã„ã¾ã™ã€‚

-----

## ğŸ¥ ãŸã¨ãˆè©±ï¼šãƒ‘ãƒ³å·¥å ´

### èƒŒæ™¯ã¨èª²é¡Œ

ãƒ‘ãƒ³ãŒç„¦ã’ã‚‹ï¼ˆæ•…éšœï¼‰ã®ã‚’é˜²ããŸã‚ã€ã‚ªãƒ¼ãƒ–ãƒ³ã«ã€ŒAIæ¸©åº¦åˆ¶å¾¡å™¨ã€ã‚’å°å…¥ã—ã¾ã—ãŸã€‚ã—ã‹ã—ã€ä»¥ä¸‹ã®èª²é¡Œã«ã‚ˆã‚Šå˜ç´”ãªæ¯”è¼ƒãŒã§ãã¾ã›ã‚“ã€‚

1.  **ç¨¼åƒç‡ã®ã°ã‚‰ã¤ã:** ãƒ•ãƒ«ç¨¼åƒã®ã‚ªãƒ¼ãƒ–ãƒ³ã¨ã€ãŸã¾ã«ã—ã‹ä½¿ã‚ãªã„ã‚ªãƒ¼ãƒ–ãƒ³ã‚’ã€Œæ™‚é–“ã€ã§æ¯”è¼ƒã™ã‚‹ã®ã¯ä¸å…¬å¹³ã§ã™ã€‚
      - *è§£æ±ºç­–:* **Normalized MTBF**ã‚’å°å‡ºã—ã¾ã™ã€‚**ã€Œå®ŸåŠ¹åˆ†æ¯ï¼ˆEffective Denominatorï¼‰ã€**ï¼ˆææ–™æ¶ˆè²»é‡/ãƒ‘ãƒ³ã®è£½é€ æ•°ï¼‰ã‚’ç”¨ã„ã¦MTBFã‚’æ­£è¦åŒ–ã—ã¾ã™ã€‚
2.  **å°å…¥æ™‚æœŸã®ãšã‚Œ:** 1æœˆå°å…¥ã€3æœˆå°å…¥ãªã©ãƒãƒ©ãƒãƒ©ã§ã™ã€‚
      - *è§£æ±ºç­–:* **ç›¸å¯¾æ™‚é–“ ($K$)** ã‚’ç”¨ã„ãŸ Staggered DiD ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ã—ã¾ã™ã€‚

#### èª¤ã£ãŸä»®å®šã¨è§£æ±ºã¸ã®çµŒç·¯ï¼šãƒ‡ãƒ¼ã‚¿æ¬ æã¸ã®å¯¾å¿œ

å½“åˆã€ã™ã¹ã¦ã®ã‚ªãƒ¼ãƒ–ãƒ³ãŒå¸¸ã«ç¨¼åƒã—ã¦ã„ã‚‹ã¨ä»®å®šã—ã¦DiDåˆ†æã‚’è¡Œã£ãŸã¨ã“ã‚ã€**çµæœãŒä¸å®‰å®šã§ãŠã‹ã—ã„ã“ã¨ã«æ°—ã¥ãã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ã™ã¹ã¦ã®è£…ç½®ãŒç­‰ã—ãç¨¼åƒã—ã¦ã„ã‚‹ã¨ã„ã†å‰æï¼ˆç¨¼åƒç‡ï¼‰ãŒé–“é•ã£ã¦ã„ãŸ**ãŸã‚ã§ã™ã€‚

**ãƒ‡ãƒ¼ã‚¿æ¬ æã¨ã„ã†èª²é¡Œ**ï¼šãã“ã§å¿…è¦ãªã€Œãƒ‘ãƒ³ã®è£½é€ æ•°ã€ãƒ‡ãƒ¼ã‚¿ãŒã€å¤ã„æœŸé–“ã‚„ä¸€éƒ¨ã®ã‚ªãƒ¼ãƒ–ãƒ³ã§æ¬ æã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸã€‚

ã“ã®èª²é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã€ç§ãŸã¡ã¯è£œåŠ©ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã™ã‚‹æ‰‹æ³•ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚**ã€Œãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æœŸé–“ã®é›»åŠ›æ¶ˆè²»é‡ï¼ˆRFæ™‚é–“ç›¸å½“ï¼‰ã€ã¨ã€Œè£½é€ æ•°ã€ã®é–¢ä¿‚ã‚’å­¦ç¿’**ã—ã€**ã‚ªãƒ¼ãƒ–ãƒ³ã®æ´—æµ„å±¥æ­´**ãªã©ã®äºŒæ¬¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¬ ææœŸé–“ã®è£½é€ æ•°ã‚’**æ¨å®š**ã™ã‚‹ã“ã¨ã§ã€ç¨¼åƒç‡è£œæ­£ã‚’å…¨æœŸé–“ã«ã‚ãŸã‚Šå¼·å›ºã«é©ç”¨ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã—ãŸã€‚

-----

## ğŸ›  åéŒ²ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åˆ†ææ‰‹æ³•

### 1\. `main_hybrid_utilization.py`ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰

**ç›®çš„:** ãƒ‡ãƒ¼ã‚¿æ¬ æã®èª²é¡Œã‚’è§£æ±ºã—ã€æ¬ æã—ãŸè£½é€ æ•°ï¼ˆãƒ‘ãƒ³ã®æ•°ï¼‰ã‚’æ¨å®šã—ã¦åŸ‹ã‚ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã§ã™ã€‚

  - **ãƒ­ã‚¸ãƒƒã‚¯:** ãƒ‡ãƒ¼ã‚¿è±Šå¯ŒãªæœŸé–“ã§\*\*ææ–™æ¶ˆè²»é‡ï¼ˆProxy Inputï¼‰**ã¨**ãƒ‘ãƒ³ã®è£½é€ æ•°ï¼ˆOutputï¼‰\*\*ã®æ¯”ç‡ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ã“ã®å­¦ç¿’ã—ãŸæ¯”ç‡ã‚’ä½¿ã„ã€Proxyãƒ‡ãƒ¼ã‚¿ã—ã‹ãªã„æœŸé–“ã®è£½é€ æ•°ã‚’æ¨å®šã—ã€åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ`Production_Imputed`ï¼‰ã‚’æº–å‚™ã—ã¾ã™ã€‚

### 2\. `main_survival_analysis.py`ï¼ˆç”Ÿå­˜æ™‚é–“åˆ†æï¼‰

æœŸé–“æŒ‡æ¨™ã¨ãƒªã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®æ¨å®šã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ç¨¼åƒç‡è£œæ­£æ¸ˆã¿ã®æœŸé–“æŒ‡æ¨™ã§ã‚ã‚‹ \*\*MTBFï¼ˆå¹³å‡æ•…éšœé–“éš”ï¼‰\*\*ã‚’ç®—å‡ºã—ã¾ã™ã€‚

#### å³æ‰“ã¡åˆ‡ã‚Šï¼ˆRight-Censoringï¼‰ã®è€ƒæ…®

åˆ†æã§ã¯ã€**å³æ‰“ã¡åˆ‡ã‚Š**ã®ãƒ‡ãƒ¼ã‚¿ã‚’è€ƒæ…®ã—ã¦ã„ã¾ã™ã€‚å³æ‰“ã¡åˆ‡ã‚Šã¨ã¯ã€è¦³æ¸¬çµ‚äº†æ™‚ç‚¹ã¾ã§ã«æ•…éšœï¼ˆã‚¤ãƒ™ãƒ³ãƒˆï¼‰ãŒç™ºç”Ÿã—ãªã‹ã£ãŸå ´åˆã‚’æŒ‡ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚‚ `Event = 0` ã¨ã—ã¦çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€æ¯é›†å›£ã®ãƒªã‚¹ã‚¯ã‚’æ­£ç¢ºã«æ¨å®šã—ã€ãƒã‚¤ã‚¢ã‚¹ã‚’é˜²ã„ã§ã„ã¾ã™ã€‚

| Model | Goal (English) | Role in this Project (Japanese) |
| :--- | :--- | :--- |
| **KMF** (Kaplan-Meier) | Estimates the **Survival Function** (non-parametric). | **è¦–è¦šçš„ãªæ¯”è¼ƒ:** æ–°æ—§ãƒ‡ãƒã‚¤ã‚¹é–“ã®MTBFï¼ˆæ•…éšœé–“éš”ï¼‰ã®åˆ†å¸ƒã‚’ã‚°ãƒ©ãƒ•ã§ç¤ºã—ã¾ã™ã€‚ |
| **Cox PH** (Proportional Hazards) | Estimates the **Hazard Ratio** (risk ratio, semi-parametric). | **ãƒªã‚¹ã‚¯ä½æ¸›ã®å®šé‡åŒ–:** æ–°ãƒ‡ãƒã‚¤ã‚¹å°å…¥ã«ã‚ˆã‚‹**æ•…éšœãƒªã‚¹ã‚¯ã®æ¸›å°‘ç‡**ã‚’æ¨å®šã—ã¾ã™ã€‚ |
| **Weibull AFT** (Accelerated Failure Time) | Estimates the **Acceleration Factor** (lifespan ratio, parametric). | **å¯¿å‘½ã®å®šé‡åŒ–:** æ–°ãƒ‡ãƒã‚¤ã‚¹å°å…¥ã«ã‚ˆã‚‹**MTBF**ã®å»¶é•·ç‡ã‚’æ¨å®šã—ã¾ã™ã€‚ |

### 3\. `main_analysis.py`ï¼ˆé™çš„DiDåˆ†æï¼‰

æ¨™æº–çš„ãªTWFEæ§‹é€ ã‚’ç”¨ã„ã€å…¨ä½“çš„ãªå¹³å‡æ²»ç™‚åŠ¹æœï¼ˆATTï¼‰ã®å®šé‡åŒ–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

#### Two-Way Fixed Effects (TWFE) ã®å½¹å‰²

TWFEãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆè£…ç½®Ã—æ™‚é–“ï¼‰ã«ãŠã‘ã‚‹**æœªè¦³æ¸¬ã®äº¤çµ¡å› å­**ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã«ã€å›ºå®šåŠ¹æœï¼ˆãƒ€ãƒŸãƒ¼å¤‰æ•°ï¼‰ã‚’å°å…¥ã—ã¾ã™ã€‚

| å›ºå®šåŠ¹æœã®ç¨®é¡ | åˆ¶å¾¡ã™ã‚‹æœªè¦³æ¸¬ã®è¦å›  | ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®å¯¾å¿œ |
| :--- | :--- | :--- |
| **Entity Fixed Effects** | è£…ç½®å›ºæœ‰ã®è¦å› ï¼ˆä¾‹ï¼šè¨­ç½®å ´æ‰€ã®ç’°å¢ƒã€åˆæœŸæ€§èƒ½ãªã©ï¼‰ | **è£…ç½®**ã”ã¨ã®å›ºå®šåŠ¹æœï¼ˆ`C(Tool)` ã¾ãŸã¯ `C(group)`ï¼‰ã«ã‚ˆã‚Šåˆ¶å¾¡ |
| **Time Fixed Effects** | å…¨è£…ç½®ã«å…±é€šã™ã‚‹è¦å› ï¼ˆä¾‹ï¼šã‚µã‚¤ãƒˆå…¨ä½“ã§ã®åœé›»ã€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä¾›çµ¦é…å»¶ãªã©ï¼‰ | **æ™‚é–“**ã”ã¨ã®å›ºå®šåŠ¹æœï¼ˆ`C(Month)` ã¾ãŸã¯ `C(Date)`ï¼‰ã«ã‚ˆã‚Šåˆ¶å¾¡ |

  - **Staggered DiDã®å®Ÿè£…:** å€‹ä½“å›ºæœ‰ã®å°å…¥æ™‚æœŸã«åˆã‚ã›ãŸ `Post` å¤‰æ•°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨­å®šãŒã€OLSã¨GLMã®ä¸¡æ–¹ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚
  - **é€£ç¶šå€¤ã®çµæœï¼ˆMTBFï¼‰:** OLSå›å¸°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
  - **ã‚«ã‚¦ãƒ³ãƒˆå€¤ã®çµæœï¼ˆRateï¼‰:** è² ã®äºŒé …åˆ†å¸ƒGLMã‚’å®Ÿè¡Œã—ã€ç¨¼åƒç‡ã®æ­£è¦åŒ–ã®ãŸã‚ã« `log(ç”Ÿç”£é‡)` ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

### 4\. `main_event_study.py`ï¼ˆå‹•çš„åˆ†æï¼‰

åŠ¹æœã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¨ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¯è¦–åŒ–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

  - **Event Study (PanelOLS):** å°å…¥å‰å¾Œã«ãŠã‘ã‚‹åŠ¹æœã®æ¨ç§»ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
  - **å¹³è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¤œè¨¼:** å°å…¥å‰ ($K < 0$) ã®ä¿‚æ•°ãŒ0ä»˜è¿‘ã§ã‚ã‚Œã°ã€æ¯”è¼ƒãŒå¦¥å½“ã§ã‚ã‚‹ã¨åˆ¤æ–­ã§ãã¾ã™ã€‚

-----

## ğŸ’» Usage

### å®Ÿè¡Œ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã‚‰å®Ÿè¡Œã™ã‚‹4ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã¨ã‚Šã¾ã™ã€‚

```bash
# ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨å®šï¼ˆæ¬ æãƒ‡ãƒ¼ã‚¿ã®è£œå®Œï¼‰
python main_hybrid_utilization.py

# ã‚¹ãƒ†ãƒƒãƒ— 2: ç”Ÿå­˜åˆ†æï¼ˆãƒªã‚¹ã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼‰ã®å®Ÿè¡Œ
python main_survival_analysis.py

# ã‚¹ãƒ†ãƒƒãƒ— 3: é™çš„DiDåˆ†æï¼ˆå› æœåŠ¹æœã®å®šé‡åŒ–ï¼‰ã®å®Ÿè¡Œ
python main_analysis.py

# ã‚¹ãƒ†ãƒƒãƒ— 4: å‹•çš„åˆ†æï¼ˆã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒ‡ã‚£ï¼‰ã®å®Ÿè¡Œ
python main_event_study.py
```

-----

## ğŸ‘¨â€ğŸ’» Author

**ä½è—¤ å‰› (Go Sato)**
ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ | å¤–è³‡ç³»åŠå°ä½“è£…ç½®ãƒ¡ãƒ¼ã‚«ãƒ¼ AIéƒ¨
å› æœæ¨è«–ã€ç”Ÿå­˜æ™‚é–“åˆ†æã€ãŠã‚ˆã³ä¿¡é ¼æ€§å·¥å­¦ã‚’å°‚é–€ã¨ã—ã¦ã„ã¾ã™ã€‚
